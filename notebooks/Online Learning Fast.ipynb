{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import *\n",
    "from scipy.sparse import *\n",
    "import similaripy as sim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from xgb_dataset_generation import adding_features\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prendiamo un elemento alla volta dal test, facciamo la similarità di quell'elemento ed estraiamo il top50. Quell'elemento poi verrà predetto. Una volta tirata fuori la predizione di quell'elemento, va aggiunto al train [di LightGBM]: non importa rifare la similarità perché valutando un elemento alla volta sappiamo già dalla similarità che facciamo se quell'elemento è nel train o no. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../dataset/original/train.csv\", escapechar=\"\\\\\")\n",
    "df_test = pd.read_csv(\"../dataset/original/test.csv\", escapechar=\"\\\\\")\n",
    "df_train = df_train.sort_values(by='record_id').reset_index(drop=True)\n",
    "df_test = df_test.sort_values(by='record_id').reset_index(drop=True)\n",
    "\n",
    "df_train.linked_id = df_train.linked_id.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['linked_id'] = df_test.record_id.str.split(\"-\")\n",
    "df_test['linked_id'] = df_test.linked_id.apply(lambda x: x[0])\n",
    "df_test.linked_id = df_test.linked_id.astype(int)\n",
    "#df_train.linked_id = df_train.linked_id.astype(int)\n",
    "only_test = set(df_test.linked_id.values) - set(df_train.linked_id.values)\n",
    "only_test_recordid = df_test[df_test.linked_id.isin(only_test)]\n",
    "df_test = df_test.drop('linked_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv(\"../dataset/validation_2/train_complete.csv\")\n",
    "train2 = pd.read_csv(\"../dataset/validation_3/train_complete.csv\")\n",
    "val = pd.read_csv(\"../dataset/validation/train_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(s, n=3):\n",
    "    s = re.sub(' +',' ',s).strip()\n",
    "    ngrams = zip(*[s[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_name(test_record, df_train):\n",
    "    df_train.name = df_train.name.astype(str)\n",
    "    test_record['name'] = test_record['name'].astype(str)\n",
    "    corpus = list(df_train.name)\n",
    "    corpus.append(test_record['name'])\n",
    "    vectorizer = CountVectorizer(preprocessor = remove_spaces, analyzer=remove_spaces)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    X_train = X[:df_train.shape[0],:]\n",
    "    X_test = X[df_train.shape[0]:,:]\n",
    "    similarity = sim.jaccard(X_test, X_train.T, k=300)\n",
    "    return similarity.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_name_fast(test_record, vectorizer, X_train):\n",
    "#     deve:\n",
    "#     - prendere il nome del test\n",
    "#     - vettorizzarlo\n",
    "#     - calcolare la similarità\n",
    "#     - ritornare la similarità con una nuova riga e una nuova colonna\n",
    "#     - ritornare X_train con una nuova riga (la vettorizzazione del nuovo record)\n",
    "#     ?\n",
    "    X_test = vectorizer.transform([test_record['name']])\n",
    "    similarity = sim.jaccard(X_test, X_train.T, k=300).tocsr()\n",
    "    return similarity.tocsr(), vstack([X_train,X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_address(test_record, df_train):\n",
    "    df_train.address = df_train.address.fillna('').astype(str)\n",
    "    test_record.address = test_record.fillna({'address':''}).address\n",
    "    corpus = list(df_train.address)\n",
    "    corpus.append(test_record.address)\n",
    "    vectorizer = CountVectorizer(preprocessor = remove_spaces, analyzer=remove_spaces)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    X_train = X[:df_train.shape[0],:]\n",
    "    X_test = X[df_train.shape[0]:,:]\n",
    "    similarity = sim.jaccard(X_test, X_train.T, k=300)\n",
    "    return similarity.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_address_fast(test_record, vectorizer, X_train):\n",
    "    test_record.address = test_record.fillna({'address':''}).address\n",
    "    X_test = vectorizer.transform([test_record.address])\n",
    "    similarity = sim.jaccard(X_test, X_train.T, k=300)\n",
    "    return similarity.tocsr(), vstack([X_train,X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_email(test_record, df_train):\n",
    "    df_train.email = df_train.email.fillna('').astype(str)\n",
    "    test_record.email = test_record.fillna({'email':''}).email\n",
    "    corpus = list(df_train.email) \n",
    "    corpus.append(test_record.email)\n",
    "    vectorizer = CountVectorizer(preprocessor = remove_spaces, analyzer=remove_spaces)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    X_train = X[:df_train.shape[0],:]\n",
    "    X_test = X[df_train.shape[0]:,:]\n",
    "    similarity = sim.jaccard(X_test, X_train.T, k=300)\n",
    "    return similarity.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_email_fast(test_record, vectorizer, X_train):\n",
    "    test_record.email = test_record.fillna({'email':''}).email\n",
    "    X_test = vectorizer.transform([test_record.email])\n",
    "    similarity = sim.jaccard(X_test, X_train.T, k=300)\n",
    "    return similarity.tocsr(), vstack([X_train,X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_phones(df_in):\n",
    "    \"\"\"\n",
    "    This functions transforms the phone column from scientific notation to readable string\n",
    "    format, e.g. 1.2933+E10 to 12933000000\n",
    "    : param df_in : the original df with the phone in scientific notation\n",
    "    : return : the clean df\n",
    "    \"\"\"\n",
    "    df = df_in.copy()\n",
    "    df.phone = df.phone.fillna('').astype(str)\n",
    "    df.phone = [p.split('.')[0] for p in df.phone]\n",
    "    return df\n",
    "\n",
    "def ngrams_phone(test_record, df_train):\n",
    "    # manually convert test_record phone\n",
    "    if np.isnan(test_record.phone):\n",
    "        test_record.phone = test_record.fillna({'phone':''}).phone\n",
    "    else:\n",
    "        test_record.phone = test_record.fillna({'phone':''}).phone.astype(str)\n",
    "    test_record.phone = test_record.phone.split('.')[0]\n",
    "    df_train = convert_phones(df_train)\n",
    "    corpus = list(df_train.phone)\n",
    "    corpus.append(test_record.phone)\n",
    "    vectorizer = CountVectorizer(preprocessor = remove_spaces, analyzer=remove_spaces)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    X_train = X[:df_train.shape[0],:]\n",
    "    X_test = X[df_train.shape[0]:,:]\n",
    "    similarity = sim.jaccard(X_test, X_train.T, k=300)\n",
    "    return similarity.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_phone_fast(test_record, vectorizer, X_train):\n",
    "    # manually convert test_record phone\n",
    "    if np.isnan(test_record.phone):\n",
    "        test_record.phone = test_record.fillna({'phone':''}).phone\n",
    "    else:\n",
    "        test_record.phone = test_record.fillna({'phone':''}).phone.astype(str)\n",
    "    test_record.phone = test_record.phone.split('.')[0]\n",
    "    X_test = vectorizer.transform([test_record.phone])\n",
    "    similarity = sim.jaccard(X_test, X_train.T, k=300)\n",
    "    return similarity.tocsr(), vstack([X_train,X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_df(df):\n",
    "    df_list = []\n",
    "    for (q, pred, pred_rec, score, s_name, s_email, s_phone, s_addr,  idx) in tqdm(\n",
    "            zip(df.queried_record_id, df.predicted_record_id, df.predicted_record_id_record, df.cosine_score,\n",
    "                df.name_cosine, df.email_cosine, df.phone_cosine, df.address_cosine, df.linked_id_idx)):\n",
    "        for x in range(len(pred)):\n",
    "            df_list.append((q, pred[x], pred_rec[x], score[x], s_name[x], s_email[x], s_phone[x], s_addr[x],  idx[x]))\n",
    "\n",
    "    # TODO da cambiare predicted_record_id in predicted_linked_id e 'predicted_record_id_record' in 'predicted_record_id'\n",
    "    df_new = pd.DataFrame(df_list, columns=['queried_record_id', 'predicted_record_id', 'predicted_record_id_record',\n",
    "                                            'cosine_score', 'name_cosine',\n",
    "                                            'email_cosine', 'phone_cosine', 'address_cosine', 'linked_id_idx',\n",
    "                                            ])\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_similarities(test_record, vectorizer_name, vectorizer_email, vectorizer_phone, vectorizer_address, \n",
    "                        X_train_name, X_train_email, X_train_phone, X_train_address, k=50):\n",
    "    \n",
    "    sim_name, X_train_name_new = ngrams_name_fast(test_record, vectorizer_name, X_train_name)\n",
    "    sim_email, X_train_email_new = ngrams_email_fast(test_record, vectorizer_email, X_train_email)\n",
    "    sim_phone, X_train_phone_new = ngrams_phone_fast(test_record, vectorizer_phone, X_train_phone)\n",
    "    sim_address, X_train_address_new = ngrams_address_fast(test_record, vectorizer_address, X_train_address)\n",
    "        \n",
    "    hybrid = sim_name + 0.2 * sim_email + 0.2 * sim_phone + 0.2 * sim_address\n",
    "    \n",
    "    linid_ = []\n",
    "    linid_idx = []\n",
    "    linid_score = []\n",
    "    linid_name_cosine = []\n",
    "    linid_email_cosine = []\n",
    "    linid_phone_cosine = []\n",
    "    linid_address_cosine = []\n",
    "    linid_record_id = []\n",
    "    \n",
    "    tr = df_train[['record_id', 'linked_id']]\n",
    "    indices = hybrid.nonzero()[1][hybrid.data.argsort()[::-1]][:k]\n",
    "    df = tr.loc[indices, :][:k]\n",
    "    linid_.append(df['linked_id'].values)\n",
    "    linid_idx.append(df.index)\n",
    "    linid_record_id.append(df.record_id.values)\n",
    "    linid_score.append(np.sort(hybrid.data)[::-1][:k]) # Questo ha senso perché tanto gli indices sono sortati in base allo scores di hybrid\n",
    "    linid_name_cosine.append([sim_name[0, t] for t in indices])\n",
    "    linid_email_cosine.append([sim_email[0, t] for t in indices])\n",
    "    linid_phone_cosine.append([sim_phone[0, t] for t in indices])\n",
    "    linid_address_cosine.append([sim_phone[0, t] for t in indices])\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['queried_record_id'] = [test_record.record_id]\n",
    "    df['predicted_record_id'] = linid_\n",
    "    df['predicted_record_id_record'] = linid_record_id\n",
    "    df['cosine_score'] = linid_score\n",
    "    df['name_cosine'] = linid_name_cosine\n",
    "    df['email_cosine'] = linid_email_cosine\n",
    "    df['phone_cosine'] = linid_phone_cosine\n",
    "    df['address_cosine'] = linid_address_cosine\n",
    "    df['linked_id_idx'] = linid_idx\n",
    "    \n",
    "    df_new = expand_df(df)\n",
    "    \n",
    "    return df_new, X_train_name_new, X_train_email_new, X_train_phone_new, X_train_address_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linked_id(new_row):\n",
    "    new_row['linked_id'] = new_row.record_id.split(\"-\")\n",
    "    new_row['linked_id'] = new_row.linked_id[0]\n",
    "    new_row['linked_id'] = int(new_row.linked_id)\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_vectorizers(df_train):\n",
    "    # vectorizer Name\n",
    "    df_train.name = df_train.name.astype(str)\n",
    "    corpus_name = list(df_train.name)\n",
    "    vectorizer_name = CountVectorizer(preprocessor = remove_spaces, analyzer=remove_spaces)\n",
    "    X_train_name = vectorizer_name.fit_transform(corpus_name)\n",
    "\n",
    "    # vectorizer Email\n",
    "    df_train.email = df_train.email.fillna('').astype(str)\n",
    "    corpus_email = list(df_train.email) \n",
    "    vectorizer_email = CountVectorizer(preprocessor = remove_spaces, analyzer=remove_spaces)\n",
    "    X_train_email = vectorizer_email.fit_transform(corpus_email)\n",
    "\n",
    "    # vectorizer Address\n",
    "    df_train.address = df_train.address.fillna('').astype(str)\n",
    "    corpus_address = list(df_train.address)\n",
    "    vectorizer_address = CountVectorizer(preprocessor = remove_spaces, analyzer=remove_spaces)\n",
    "    X_train_address = vectorizer_address.fit_transform(corpus_address)\n",
    "\n",
    "    # vectorizer Phone\n",
    "    df_train = convert_phones(df_train)\n",
    "    corpus_phone = list(df_train.phone)\n",
    "    vectorizer_phone = CountVectorizer(preprocessor = remove_spaces, analyzer=remove_spaces)\n",
    "    X_train_phone = vectorizer_phone.fit_transform(corpus_phone)\n",
    "    \n",
    "    return vectorizer_name, vectorizer_email, vectorizer_address, vectorizer_phone, X_train_name, X_train_email, X_train_address, X_train_phone "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_name, vectorizer_email, vectorizer_address, vectorizer_phone, X_train_name, X_train_email, X_train_address, X_train_phone= create_all_vectorizers(df_train)\n",
    "t1 = time.time()\n",
    "test_record_exp, X_train_name_new, X_train_email_new, X_train_phone_new, X_train_address_new = expand_similarities(df_test.loc[2], vectorizer_name, vectorizer_email, vectorizer_phone, vectorizer_address, \n",
    "                        X_train_name, X_train_email, X_train_phone, X_train_address)\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_record_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sequential Test Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dobbiamo creare un test set sequenziale [con le relative risposte], in modo che se ci sono due record che fanno riferimento allo stesso linked_id che però sono presenti solo nel test, il primo che arriva non ha riferimenti nel train ed è impossibile predirlo correttamente; viene però aggiunto al train, dunque quando il secondo record deve essere valutato, allora la risposta corretta corrisponde a quello precedentemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_train = df_train.groupby('linked_id').apply(lambda x: list(x['record_id'])).reset_index().rename(columns={0:'record_ids'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['linked_id'] = df_test.record_id.str.split('-')\n",
    "df_test['linked_id'] = df_test.linked_id.apply(lambda x: x[0])\n",
    "df_test.linked_id = df_test.linked_id.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.merge(group_train, how= 'left', on='linked_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_labelling(df):\n",
    "    linked_seen = []\n",
    "    new_group = { }\n",
    "    new_df = []\n",
    "    df_notna = df[~df.record_ids.isna()]\n",
    "    df_na = df[df.record_ids.isna()]\n",
    "    relevant_idx= []\n",
    "    for (i, l, r) in tqdm(zip(df_na.index, df_na.linked_id, df_na.record_id)):\n",
    "        if l not in linked_seen:\n",
    "            linked_seen.append(l)\n",
    "            new_df.append((i, r, np.nan))\n",
    "            new_group[l] = [r]\n",
    "        else:\n",
    "            current_group = new_group[l]\n",
    "            #print(f'{r} and the group: {current_group}')\n",
    "            new_df.append((i, r, current_group))\n",
    "            new_group[l].append(r)\n",
    "            relevant_idx.append(i)\n",
    "    res = pd.DataFrame(new_df, columns=['index','record_id', 'record_ids']).set_index('index')\n",
    "    full_df = pd.concat([df_notna, res])\n",
    "    full_df = full_df.sort_index()\n",
    "    return full_df, relevant_idx\n",
    "\n",
    "def get_target(df):\n",
    "    df, idx = seq_labelling(df)\n",
    "    df_notidx = df.loc[~df.index.isin(idx)]\n",
    "    df = df.loc[idx]\n",
    "    new_df = []\n",
    "    for (i,r, g) in tqdm(zip(df.index, df.record_id, df.record_ids)):\n",
    "        idx = g.index(r)\n",
    "        g = g[:idx]\n",
    "        new_df.append((i, r, g))\n",
    "    new_df = pd.DataFrame(new_df, columns=['index','record_id', 'record_ids']).set_index('index')\n",
    "    res = pd.concat([new_df, df_notidx])\n",
    "    res = res.sort_index()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = get_target(df_test[['record_id', 'linked_id','record_ids']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nota\n",
    "Invece di fare le predizioni su tutto il test, visto che sappiamo come performa l'algoritmo classico su quei record che hanno riferimenti nel train, possiamo restringere l'analisi ai record del test che non hanno riferimenti nel train e che hanno duplicati all'interno del test stesso [per quelli che non sono duplicati non ha senso fare tale analisi perché non forniscono nessun contributo alle query successive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_df = only_test_recordid[only_test_recordid.duplicated('linked_id', keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = duplicates_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_preds(preds):\n",
    "    ordered_lin = []\n",
    "    ordered_score = []\n",
    "    ordered_record = []\n",
    "    for i in range(len(preds)):\n",
    "        l = sorted(preds[i], key=lambda t: t[1], reverse=True)\n",
    "        lin = [x[0] for x in l]\n",
    "        s = [x[1] for x in l]\n",
    "        r = [x[2] for x in l]\n",
    "        ordered_lin.append(lin)\n",
    "        ordered_score.append(s)\n",
    "        ordered_record.append(r)\n",
    "    return ordered_lin, ordered_score, ordered_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgfsd = vectorizer_name.transform(['vgfsd'])\n",
    "vstack([X_train_name,vgfsd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load training set\n",
    "df_train = pd.read_csv(\"../dataset/original/train.csv\", escapechar=\"\\\\\")\n",
    "df_train = df_train.sort_values(by='record_id').reset_index(drop=True)\n",
    "df_train.linked_id = df_train.linked_id.astype(int)\n",
    "# get vectorizers\n",
    "vectorizer_name, vectorizer_email, vectorizer_address, vectorizer_phone, X_train_name, X_train_email, X_train_address, X_train_phone= create_all_vectorizers(df_train)\n",
    "\n",
    "# Online Learning    -------------> TODO: ci mette troppo, circa 7ore\n",
    "test_prediction = pd.DataFrame()\n",
    "seen = []\n",
    "\n",
    "for i in tqdm(idx[:10]):\n",
    "    test_row = duplicates_df.loc[i]\n",
    "    print(f'Record: {test_row.record_id}')\n",
    "    t1 = time.time()\n",
    "    #print(f'Extract test record at index: {i}')\n",
    "    # Get the test record to be evaluate\n",
    "    #print('Create Features')\n",
    "    test_row_exp, X_train_name, X_train_email, X_train_phone, X_train_address = expand_similarities(test_row, vectorizer_name, vectorizer_email, vectorizer_phone, vectorizer_address, \n",
    "                        X_train_name, X_train_email, X_train_phone, X_train_address)\n",
    "    test_row_exp = adding_features(test_row_exp, isValidation=False, path=os.path.join('..', 'dataset', 'original'), incremental_train=df_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    #print('Get Predictions')\n",
    "    predictions = ranker.predict(test_row_exp.drop(['queried_record_id','linked_id_idx', 'predicted_record_id','predicted_record_id_record'], axis=1))\n",
    "    test_row_exp['predictions'] = predictions\n",
    "    df_predictions = test_row_exp[['queried_record_id', 'predicted_record_id', 'predicted_record_id_record', 'predictions']]\n",
    "    \n",
    "    # Re-order predictions\n",
    "    #print('Reorder Predictions')\n",
    "    rec_pred = []\n",
    "    for (l,p,record_id) in zip(df_predictions.predicted_record_id, df_predictions.predictions, df_predictions.predicted_record_id_record):\n",
    "        rec_pred.append((l, p, record_id))\n",
    "\n",
    "    df_predictions['rec_pred'] = rec_pred\n",
    "    group_queried = df_predictions[['queried_record_id', 'rec_pred']].groupby('queried_record_id').apply(lambda x: list(x['rec_pred']))\n",
    "    df_predictions = pd.DataFrame(group_queried).reset_index().rename(columns={0 : 'rec_pred'})\n",
    "    \n",
    "    # Store predictions\n",
    "    #print('Store Predictions')\n",
    "    df_predictions['ordered_linked'], df_predictions['ordered_scores'], df_predictions['ordered_record'] = reorder_preds(df_predictions.rec_pred.values)\n",
    "    test_prediction = pd.concat([test_prediction, df_predictions], ignore_index=True)\n",
    "    new_row = get_linked_id(test_row)\n",
    "    df_train = df_train.append(new_row, ignore_index=True)\n",
    "    t2 = time.time()\n",
    "    #print(f'Iteration completed in {t2 - t1}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idea era che una volta calcolate le predizioni 'sequenziali', possiamo guardare quanto queste siano corrette matchandole con seq['record_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemi:\n",
    "- ci mette davvero troppo, soprattutto il calcolo delle similarità ed estrazione delle topN, trovare un modo più rapido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
